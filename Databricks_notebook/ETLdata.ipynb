{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb418a07-5f1c-4f73-be3b-4ecd6c73c949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pyodbc\n",
    "from collections import defaultdict\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from loguru import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2874cab6-92ee-4ad2-a6ab-ab4310020915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"pipeline_name\", \"\")\n",
    "dbutils.widgets.text(\"pipeline_run_id\", \"\")\n",
    "dbutils.widgets.text(\"process_name\", \"\")\n",
    "dbutils.widgets.text(\"File_Names\", \"\")\n",
    "dbutils.widgets.text(\"landing_path\", \"\")\n",
    "dbutils.widgets.text(\"curated_path\", \"\")\n",
    "dbutils.widgets.text(\"processed_path\", \"\")\n",
    "dbutils.widgets.text(\"status\", \"\")\n",
    "dbutils.widgets.text(\"Mode\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee5c97a-f102-4fa5-be3a-2e35fb2f76a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_name = dbutils.widgets.get(\"pipeline_name\").strip()\n",
    "pipeline_run_id = dbutils.widgets.get(\"pipeline_run_id\").strip()\n",
    "process_name = dbutils.widgets.get(\"process_name\").strip()\n",
    "File_Names = dbutils.widgets.get(\"File_Names\").strip()\n",
    "landing_path = dbutils.widgets.get(\"landing_path\").strip(\"/\")\n",
    "curated_path = dbutils.widgets.get(\"curated_path\").strip()\n",
    "process_path = dbutils.widgets.get(\"processed_path\").strip()\n",
    "status = dbutils.widgets.get(\"status\")\n",
    "Mode=dbutils.widgets.get(\"Mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dab64f5-f692-479b-929e-7841be956414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PipelineLogger:\n",
    "    def __init__(self, pipeline_name, pipeline_run_id):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.pipeline_run_id = pipeline_run_id\n",
    "\n",
    "    def _connect(self):\n",
    "        server = 'etlsqlservers.database.windows.net'\n",
    "        database = 'ETLdatabases'\n",
    "        username = dbutils.secrets.get(scope='Azr-adf-scope', key='UID')\n",
    "        password = dbutils.secrets.get(scope='Azr-adf-scope', key='PWD')\n",
    "        return pyodbc.connect(\n",
    "            f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "        )\n",
    "\n",
    "    def log_start_time(self):\n",
    "        try:\n",
    "            conn = self._connect()\n",
    "            cursor = conn.cursor()\n",
    "            start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO metadata.pipeline_runs (pipeline_run_id, pipeline_name, start_time)\n",
    "                VALUES (?, ?, ?)\n",
    "            \"\"\", (self.pipeline_run_id, self.pipeline_name, start_time))\n",
    "            conn.commit()\n",
    "            logger.info(\"successs\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"error\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "\n",
    "    def log_end_time(self, status):\n",
    "        try:\n",
    "            conn = self._connect()\n",
    "            cursor = conn.cursor()\n",
    "            end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            logger.info(f\"End time: {end_time}, Status: {status}, Run ID: {self.pipeline_run_id}, Pipeline: {self.pipeline_name}\")\n",
    "            cursor.execute(\"\"\"\n",
    "                UPDATE metadata.pipeline_runs\n",
    "                SET end_time = ?, status = ?\n",
    "                WHERE pipeline_run_id = ? AND pipeline_name = ?\n",
    "            \"\"\", (end_time, status, self.pipeline_run_id, self.pipeline_name))\n",
    "            conn.commit()\n",
    "            logger.info(\"Successfully updated end time and status.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error updating end time/status: {e}\")\n",
    "        finally:\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "            \n",
    "    def load_and_transform_table(self,File_Names, landing_path, column_meta_by_table):\n",
    "        logger.info(f\"Processing table: {File_Names}\")\n",
    "        df = spark.read.parquet(landing_path)\n",
    "        mappings = column_meta_by_table.get(File_Names, [])\n",
    "\n",
    "        sql_to_spark_type = {\n",
    "            \"int\": IntegerType(),\n",
    "            \"string\": StringType(),\n",
    "            \"float\": FloatType(),\n",
    "            \"double\": DoubleType(),\n",
    "            \"date\": DateType(),\n",
    "            \"timestamp\": TimestampType(),\n",
    "            \"varchar(500)\": StringType()\n",
    "        }\n",
    "\n",
    "        for col_map in mappings:\n",
    "            src = col_map[\"source_column_name\"]\n",
    "            dst = col_map[\"destination_column_name\"]\n",
    "            dtype = sql_to_spark_type.get(col_map[\"destination_column_data_type\"], StringType())\n",
    "\n",
    "            if src in df.columns:\n",
    "                df = df.withColumn(src, col(src).cast(dtype))\n",
    "                if src != dst:\n",
    "                    df = df.withColumnRenamed(src, dst)\n",
    "            else:\n",
    "                logger.warning(f\"Column not found in the DataFrame: {src}\")\n",
    "        df = df.dropDuplicates()\n",
    "        return df\n",
    "\n",
    "# Main method\n",
    "    def run_dqm_validation(self, File_Names, landing_path, curated_path):\n",
    "        try:\n",
    "            df = spark.read.format(\"parquet\").load(landing_path)\n",
    "\n",
    "            conn = self._connect()\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT source_table_name, source_column_name, destination_column_name, destination_column_data_type \n",
    "                FROM metadata.column_meta\n",
    "            \"\"\")\n",
    "            rows = cursor.fetchall()\n",
    "\n",
    "            column_meta_by_table = defaultdict(list)\n",
    "            for row in rows:\n",
    "                column_meta_by_table[row.source_table_name].append({\n",
    "                    \"source_column_name\": row.source_column_name,\n",
    "                    \"destination_column_name\": row.destination_column_name,\n",
    "                    \"destination_column_data_type\": row.destination_column_data_type.lower()\n",
    "                })\n",
    "\n",
    "            transformed_df = self.load_and_transform_table(File_Names, landing_path, column_meta_by_table)\n",
    "\n",
    "            output_path = f\"{curated_path}/{File_Names}\"\n",
    "            transformed_df.write.format(\"delta\").mode(Mode).option(\"mergeSchema\", \"true\").save(output_path)\n",
    "\n",
    "            logger.info(\"Success\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"DQM Failed: {str(e)}\")\n",
    "    \n",
    "    def archive_path(self, landing_path, File_Names):\n",
    "        try: # e.g. container/landing/mkpf\n",
    "            dst_dir = f\"dbfs:/mnt/ETLmountpoint/archive/{File_Names}\"  # e.g. .../archive/mkpf\n",
    "            logger.info(f\"Moving directory {landing_path} to {dst_dir}\")\n",
    "\n",
    "        # Move the whole directory recursively\n",
    "            dbutils.fs.mv(landing_path, dst_dir, recurse=True)\n",
    "\n",
    "            logger.info(f\"Successfully moved {landing_path} to {dst_dir}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to move directory {File_Names}: {str(e)}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118d669d-fac7-47dd-906d-f7e311544f55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    function= PipelineLogger(pipeline_name, pipeline_run_id)\n",
    "\n",
    "    try:\n",
    "        if process_name == \"start_time\":\n",
    "            function.log_start_time()\n",
    "        elif process_name == \"l2c\":\n",
    "            function.run_dqm_validation(File_Names, landing_path, curated_path)\n",
    "        elif process_name == \"end_time\":\n",
    "            function.log_end_time(status)\n",
    "        elif process_name == \"archive\":\n",
    "            function.archive_path(landing_path, File_Names)\n",
    "        else:\n",
    "            logger.info(\"Invalid mode. Use: start, dqm, end.\")\n",
    "    except Exception as ex:\n",
    "        logger.error(f\"Pipeline failed with error: {ex}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5328896496693368,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "ETLdata",
   "widgets": {
    "File_Names": {
     "currentValue": "",
     "nuid": "1fd90e11-7895-4fc6-8f3e-9a2d2d2afb5c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "File_Names",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "File_Names",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "curated_path": {
     "currentValue": "",
     "nuid": "472a788a-abde-444a-bbdc-52c0a0d78ea1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "curated_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "curated_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "landing_path": {
     "currentValue": "",
     "nuid": "fcdae6d5-1bc0-4f37-84ef-fe55782de014",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "landing_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "landing_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "pipeline_name": {
     "currentValue": "",
     "nuid": "389e10d8-6d23-44ad-b798-d6ef14ef575a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "pipeline_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "pipeline_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "pipeline_run_id": {
     "currentValue": "",
     "nuid": "ff1d6a9f-39a9-4b8d-a006-b879d91540b4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "pipeline_run_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "pipeline_run_id",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "process_name": {
     "currentValue": "",
     "nuid": "8b7b77b9-7430-405a-8ca2-517f10c68a3a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "process_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "process_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
